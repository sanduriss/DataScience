{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework6.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanduriss/DataScience/blob/main/Homework6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 (30 Points)\n",
        "\n",
        "A sample of 30 respondents was interviewed using mall intercept interviewing. The respondents were asked to indicate their degree of agreement with the following statements using a seven-point scale (1 = strongly disagree, 7 = strongly agree).\n",
        "\n",
        "•\tV1 = It is important to buy a toothpaste that prevents cavities\n",
        "\n",
        "•\tV2 = I like a toothpaste that gives a shiny teeth\n",
        "\n",
        "•\tV3 = A toothpaste should strengthen your gums teeth\n",
        "\n",
        "•\tV4 = I prefer a toothpaste that freshens breath\n",
        "\n",
        "•\tV5 = Prevention of tooth decay is not an important benefit offered by a toothpaste\n",
        "\n",
        "•\tV6 = The most important consideration in buying a toothpaste is attractive teeth\n",
        "\n",
        "By using the variables,\n",
        "\n",
        "(1) How many factors is extracted from data for principal component analysis?\n",
        "\n",
        "(2) What is the total percentage of variance explained by these principal components?\n",
        "\n",
        "(3)Which variables are included in the same factors ? How can you name these factors?\n",
        "\n",
        "You can find the data at \n",
        "\n",
        "https://raw.githubusercontent.com/ogut77/DataScience/main/data/Toothpaste.csv"
      ],
      "metadata": {
        "id": "54_BmoeeZfYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1**"
      ],
      "metadata": {
        "id": "GIe0Bkq7tW8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.1"
      ],
      "metadata": {
        "id": "pBM6LC3stbOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "df=pd.read_csv('https://raw.githubusercontent.com/ogut77/DataScience/main/data/Toothpaste.csv')\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X= StandardScaler().fit_transform(df)\n",
        "X=pd.DataFrame(X)\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=6)\n",
        "principalComponents = pca.fit_transform(X)\n",
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "lbIbCSVEtWIW",
        "outputId": "c21946c3-4021-469f-8b8e-6d9edb6a56ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.39747021, 0.30295205, 0.16145572, 0.07131525, 0.05138315,\n",
              "       0.01542362])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on our variables, our benchmark is 1/6 = 0.16666667.\n",
        "So we have 2 factors extracted from the data for principal component analysis.\n"
      ],
      "metadata": {
        "id": "2x1Wht2jum1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.2"
      ],
      "metadata": {
        "id": "b3p_S-BSxi0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0.39747021 + 0.30295205 = 0.70042226**"
      ],
      "metadata": {
        "id": "Z4vgkvS9xkxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2\n",
        "import pandas as pd\n",
        "breast_dataset=pd.read_csv('https://raw.githubusercontent.com/ogut77/DataScience/main/data/Bcancer.csv')\n",
        "breast_dataset\n",
        "y=breast_dataset['label']\n",
        "X=breast_dataset.drop(['label'], axis = 1)\n",
        "X"
      ],
      "metadata": {
        "id": "M-SHEIK8i9Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2) (70 points)Using breast cancer data above, \n",
        "\n",
        "(1) split the data two : training (65%) and testing (35%)\n",
        "\n",
        "(2) Using logistic regression model in train data, get the performance metric on test data(accuracy,recall, precision confusuion matrix)\n",
        "\n",
        "(3)Using knn model in train data(choose k based on CV), get the performance metric on test data(accuracy,recall, precision confusuion matrix). What is the k value chosen based on k?\n",
        "\n",
        "(4)Using naive  model in train data, get the performance metric on test data(accuracy,recall, precision confusuion matrix)\n",
        "\n",
        "(5)Using SVM with parameter tuning(you can use optuna) in train data ,get the performance metric on test data(accuracy,recall, precision confusuion matrix)"
      ],
      "metadata": {
        "id": "zC7aVfnCjG09"
      }
    }
  ]
}